{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "authorship_tag": "ABX9TyPhP3G4cJ11dy3He4tFXccJ"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "LpXCMMR7SI_Z",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764766725875,
     "user_tz": -480,
     "elapsed": 64570,
     "user": {
      "displayName": "joker",
      "userId": "03249235188414976044"
     }
    },
    "outputId": "a52ef97c-cb61-448c-847e-34505e5908e8"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cloning into 'LLaMA-Factory'...\n",
      "remote: Enumerating objects: 24605, done.\u001B[K\n",
      "remote: Counting objects: 100% (125/125), done.\u001B[K\n",
      "remote: Compressing objects: 100% (103/103), done.\u001B[K\n",
      "remote: Total 24605 (delta 63), reused 22 (delta 22), pack-reused 24480 (from 2)\u001B[K\n",
      "Receiving objects: 100% (24605/24605), 12.22 MiB | 17.21 MiB/s, done.\n",
      "Resolving deltas: 100% (17694/17694), done.\n",
      "/content/LLaMA-Factory\n",
      "Obtaining file:///content/LLaMA-Factory\n",
      "  Installing build dependencies ... \u001B[?25l\u001B[?25hdone\n",
      "  Checking if build backend supports build_editable ... \u001B[?25l\u001B[?25hdone\n",
      "  Getting requirements to build editable ... \u001B[?25l\u001B[?25hdone\n",
      "  Preparing editable metadata (pyproject.toml) ... \u001B[?25l\u001B[?25hdone\n",
      "Collecting transformers!=4.52.0,!=4.57.0,<=4.57.1,>=4.49.0 (from llamafactory==0.9.4.dev0)\n",
      "  Downloading transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m44.0/44.0 kB\u001B[0m \u001B[31m2.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: datasets<=4.0.0,>=2.16.0 in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.4.dev0) (4.0.0)\n",
      "Collecting accelerate<=1.11.0,>=1.3.0 (from llamafactory==0.9.4.dev0)\n",
      "  Downloading accelerate-1.11.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting peft<=0.17.1,>=0.14.0 (from llamafactory==0.9.4.dev0)\n",
      "  Downloading peft-0.17.1-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting trl<=0.9.6,>=0.8.6 (from llamafactory==0.9.4.dev0)\n",
      "  Downloading trl-0.9.6-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting gradio<=5.45.0,>=4.38.0 (from llamafactory==0.9.4.dev0)\n",
      "  Downloading gradio-5.45.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.4.dev0) (3.10.0)\n",
      "Collecting tyro<0.9.0 (from llamafactory==0.9.4.dev0)\n",
      "  Downloading tyro-0.8.14-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.4.dev0) (0.8.1)\n",
      "Collecting numpy<2.0.0 (from llamafactory==0.9.4.dev0)\n",
      "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m61.0/61.0 kB\u001B[0m \u001B[31m5.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.4.dev0) (2.2.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.4.dev0) (1.16.3)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.4.dev0) (0.2.1)\n",
      "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.4.dev0) (0.12.0)\n",
      "Collecting modelscope>=1.14.0 (from llamafactory==0.9.4.dev0)\n",
      "  Downloading modelscope-1.32.0-py3-none-any.whl.metadata (43 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m43.3/43.3 kB\u001B[0m \u001B[31m4.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: hf-transfer in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.4.dev0) (0.1.9)\n",
      "Collecting safetensors<=0.5.3 (from llamafactory==0.9.4.dev0)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting fire (from llamafactory==0.9.4.dev0)\n",
      "  Downloading fire-0.7.1-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: omegaconf in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.4.dev0) (2.3.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.4.dev0) (25.0)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.4.dev0) (5.29.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.4.dev0) (6.0.3)\n",
      "Collecting pydantic<=2.10.6 (from llamafactory==0.9.4.dev0)\n",
      "  Downloading pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\n",
      "Requirement already satisfied: uvicorn in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.4.dev0) (0.38.0)\n",
      "Requirement already satisfied: fastapi in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.4.dev0) (0.118.3)\n",
      "Requirement already satisfied: sse-starlette in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.4.dev0) (3.0.3)\n",
      "Collecting av (from llamafactory==0.9.4.dev0)\n",
      "  Downloading av-16.0.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: librosa in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.4.dev0) (0.11.0)\n",
      "Requirement already satisfied: propcache!=0.4.0 in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.4.dev0) (0.4.1)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.4.dev0) (2.9.0+cu126)\n",
      "Requirement already satisfied: torchvision>=0.15.0 in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.4.dev0) (0.24.0+cu126)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.4.dev0) (3.9.1)\n",
      "Requirement already satisfied: jieba in /usr/local/lib/python3.12/dist-packages (from llamafactory==0.9.4.dev0) (0.42.1)\n",
      "Collecting rouge-chinese (from llamafactory==0.9.4.dev0)\n",
      "  Downloading rouge_chinese-1.0.3-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate<=1.11.0,>=1.3.0->llamafactory==0.9.4.dev0) (5.9.5)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate<=1.11.0,>=1.3.0->llamafactory==0.9.4.dev0) (0.36.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets<=4.0.0,>=2.16.0->llamafactory==0.9.4.dev0) (3.20.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets<=4.0.0,>=2.16.0->llamafactory==0.9.4.dev0) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets<=4.0.0,>=2.16.0->llamafactory==0.9.4.dev0) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets<=4.0.0,>=2.16.0->llamafactory==0.9.4.dev0) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets<=4.0.0,>=2.16.0->llamafactory==0.9.4.dev0) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets<=4.0.0,>=2.16.0->llamafactory==0.9.4.dev0) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets<=4.0.0,>=2.16.0->llamafactory==0.9.4.dev0) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.0.0,>=2.16.0->llamafactory==0.9.4.dev0) (2025.3.0)\n",
      "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (24.1.0)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (4.11.0)\n",
      "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (1.2.0)\n",
      "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (1.0.0)\n",
      "Collecting gradio-client==1.13.0 (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0)\n",
      "  Downloading gradio_client-1.13.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (0.1.2)\n",
      "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (0.28.1)\n",
      "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (3.1.6)\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (3.0.3)\n",
      "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (3.11.4)\n",
      "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (11.3.0)\n",
      "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (0.0.20)\n",
      "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (0.14.6)\n",
      "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (0.1.7)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (2.10.0)\n",
      "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (0.48.0)\n",
      "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (0.13.3)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (0.20.0)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.12/dist-packages (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (4.15.0)\n",
      "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.13.0->gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (15.0.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.4.dev0) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.4.dev0) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.4.dev0) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.4.dev0) (1.4.9)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.4.dev0) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.4.dev0) (2.9.0.post0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from modelscope>=1.14.0->llamafactory==0.9.4.dev0) (75.2.0)\n",
      "Requirement already satisfied: urllib3>=1.26 in /usr/local/lib/python3.12/dist-packages (from modelscope>=1.14.0->llamafactory==0.9.4.dev0) (2.5.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.0.0->llamafactory==0.9.4.dev0) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.0.0->llamafactory==0.9.4.dev0) (2025.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<=2.10.6->llamafactory==0.9.4.dev0) (0.7.0)\n",
      "Collecting pydantic-core==2.27.2 (from pydantic<=2.10.6->llamafactory==0.9.4.dev0)\n",
      "  Downloading pydantic_core-2.27.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->llamafactory==0.9.4.dev0) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->llamafactory==0.9.4.dev0) (3.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->llamafactory==0.9.4.dev0) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->llamafactory==0.9.4.dev0) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->llamafactory==0.9.4.dev0) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->llamafactory==0.9.4.dev0) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->llamafactory==0.9.4.dev0) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->llamafactory==0.9.4.dev0) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->llamafactory==0.9.4.dev0) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->llamafactory==0.9.4.dev0) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->llamafactory==0.9.4.dev0) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->llamafactory==0.9.4.dev0) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->llamafactory==0.9.4.dev0) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->llamafactory==0.9.4.dev0) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->llamafactory==0.9.4.dev0) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->llamafactory==0.9.4.dev0) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->llamafactory==0.9.4.dev0) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->llamafactory==0.9.4.dev0) (3.5.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers!=4.52.0,!=4.57.0,<=4.57.1,>=4.49.0->llamafactory==0.9.4.dev0) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers!=4.52.0,!=4.57.0,<=4.57.1,>=4.49.0->llamafactory==0.9.4.dev0) (0.22.1)\n",
      "Requirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.12/dist-packages (from tyro<0.9.0->llamafactory==0.9.4.dev0) (0.17.0)\n",
      "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.12/dist-packages (from tyro<0.9.0->llamafactory==0.9.4.dev0) (13.9.4)\n",
      "Collecting shtab>=1.5.6 (from tyro<0.9.0->llamafactory==0.9.4.dev0)\n",
      "  Downloading shtab-1.8.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.12/dist-packages (from uvicorn->llamafactory==0.9.4.dev0) (8.3.1)\n",
      "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.12/dist-packages (from uvicorn->llamafactory==0.9.4.dev0) (0.16.0)\n",
      "Requirement already satisfied: termcolor in /usr/local/lib/python3.12/dist-packages (from fire->llamafactory==0.9.4.dev0) (3.2.0)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.12/dist-packages (from librosa->llamafactory==0.9.4.dev0) (3.1.0)\n",
      "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.12/dist-packages (from librosa->llamafactory==0.9.4.dev0) (0.60.0)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from librosa->llamafactory==0.9.4.dev0) (1.6.1)\n",
      "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa->llamafactory==0.9.4.dev0) (1.5.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from librosa->llamafactory==0.9.4.dev0) (4.4.2)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.12/dist-packages (from librosa->llamafactory==0.9.4.dev0) (0.13.1)\n",
      "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.12/dist-packages (from librosa->llamafactory==0.9.4.dev0) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.12/dist-packages (from librosa->llamafactory==0.9.4.dev0) (1.0.0)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.12/dist-packages (from librosa->llamafactory==0.9.4.dev0) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa->llamafactory==0.9.4.dev0) (1.1.2)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/dist-packages (from omegaconf->llamafactory==0.9.4.dev0) (4.9.3)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from rouge-chinese->llamafactory==0.9.4.dev0) (1.17.0)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (3.11)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (1.3.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.0.0,>=2.16.0->llamafactory==0.9.4.dev0) (3.13.2)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (1.0.9)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate<=1.11.0,>=1.3.0->llamafactory==0.9.4.dev0) (1.2.0)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.51.0->librosa->llamafactory==0.9.4.dev0) (0.43.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.1->librosa->llamafactory==0.9.4.dev0) (4.5.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets<=4.0.0,>=2.16.0->llamafactory==0.9.4.dev0) (3.4.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.4.dev0) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.4.dev0) (2.19.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.1.0->librosa->llamafactory==0.9.4.dev0) (3.6.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile>=0.12.1->librosa->llamafactory==0.9.4.dev0) (2.0.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->llamafactory==0.9.4.dev0) (1.3.0)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (1.5.4)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.0.0,>=2.16.0->llamafactory==0.9.4.dev0) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.0.0,>=2.16.0->llamafactory==0.9.4.dev0) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.0.0,>=2.16.0->llamafactory==0.9.4.dev0) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.0.0,>=2.16.0->llamafactory==0.9.4.dev0) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.0.0,>=2.16.0->llamafactory==0.9.4.dev0) (6.7.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.0.0,>=2.16.0->llamafactory==0.9.4.dev0) (1.22.0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa->llamafactory==0.9.4.dev0) (2.23)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.4.dev0) (0.1.2)\n",
      "Downloading accelerate-1.11.0-py3-none-any.whl (375 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m375.8/375.8 kB\u001B[0m \u001B[31m3.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading gradio-5.45.0-py3-none-any.whl (60.4 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m60.4/60.4 MB\u001B[0m \u001B[31m18.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading gradio_client-1.13.0-py3-none-any.whl (325 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m325.0/325.0 kB\u001B[0m \u001B[31m26.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading modelscope-1.32.0-py3-none-any.whl (6.0 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m6.0/6.0 MB\u001B[0m \u001B[31m62.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m18.0/18.0 MB\u001B[0m \u001B[31m90.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading peft-0.17.1-py3-none-any.whl (504 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m504.9/504.9 kB\u001B[0m \u001B[31m38.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m431.7/431.7 kB\u001B[0m \u001B[31m29.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading pydantic_core-2.27.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.0/2.0 MB\u001B[0m \u001B[31m58.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m471.6/471.6 kB\u001B[0m \u001B[31m27.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m12.0/12.0 MB\u001B[0m \u001B[31m106.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading trl-0.9.6-py3-none-any.whl (245 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m245.8/245.8 kB\u001B[0m \u001B[31m22.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading tyro-0.8.14-py3-none-any.whl (109 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m109.8/109.8 kB\u001B[0m \u001B[31m11.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading av-16.0.1-cp312-cp312-manylinux_2_28_x86_64.whl (40.5 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m40.5/40.5 MB\u001B[0m \u001B[31m13.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading fire-0.7.1-py3-none-any.whl (115 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m115.9/115.9 kB\u001B[0m \u001B[31m12.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading rouge_chinese-1.0.3-py3-none-any.whl (21 kB)\n",
      "Downloading shtab-1.8.0-py3-none-any.whl (14 kB)\n",
      "Building wheels for collected packages: llamafactory\n",
      "  Building editable for llamafactory (pyproject.toml) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for llamafactory: filename=llamafactory-0.9.4.dev0-0.editable-py3-none-any.whl size=28938 sha256=f49fd995725e916cd198b6283b1a4d1229488d283918de21bf2b42eba2893c81\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-e8aa2rg6/wheels/68/8b/5e/52f9888e6a91a2651260d603137c052b925af896da6e32a3f7\n",
      "Successfully built llamafactory\n",
      "Installing collected packages: shtab, safetensors, rouge-chinese, pydantic-core, numpy, fire, av, pydantic, modelscope, tyro, gradio-client, transformers, gradio, accelerate, trl, peft, llamafactory\n",
      "  Attempting uninstall: safetensors\n",
      "    Found existing installation: safetensors 0.7.0\n",
      "    Uninstalling safetensors-0.7.0:\n",
      "      Successfully uninstalled safetensors-0.7.0\n",
      "  Attempting uninstall: pydantic-core\n",
      "    Found existing installation: pydantic_core 2.41.4\n",
      "    Uninstalling pydantic_core-2.41.4:\n",
      "      Successfully uninstalled pydantic_core-2.41.4\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.0.2\n",
      "    Uninstalling numpy-2.0.2:\n",
      "      Successfully uninstalled numpy-2.0.2\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 2.12.3\n",
      "    Uninstalling pydantic-2.12.3:\n",
      "      Successfully uninstalled pydantic-2.12.3\n",
      "  Attempting uninstall: gradio-client\n",
      "    Found existing installation: gradio_client 1.14.0\n",
      "    Uninstalling gradio_client-1.14.0:\n",
      "      Successfully uninstalled gradio_client-1.14.0\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.57.2\n",
      "    Uninstalling transformers-4.57.2:\n",
      "      Successfully uninstalled transformers-4.57.2\n",
      "  Attempting uninstall: gradio\n",
      "    Found existing installation: gradio 5.50.0\n",
      "    Uninstalling gradio-5.50.0:\n",
      "      Successfully uninstalled gradio-5.50.0\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 1.12.0\n",
      "    Uninstalling accelerate-1.12.0:\n",
      "      Successfully uninstalled accelerate-1.12.0\n",
      "  Attempting uninstall: peft\n",
      "    Found existing installation: peft 0.18.0\n",
      "    Uninstalling peft-0.18.0:\n",
      "      Successfully uninstalled peft-0.18.0\n",
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
      "mcp 1.22.0 requires pydantic<3.0.0,>=2.11.0, but you have pydantic 2.10.6 which is incompatible.\u001B[0m\u001B[31m\n",
      "\u001B[0mSuccessfully installed accelerate-1.11.0 av-16.0.1 fire-0.7.1 gradio-5.45.0 gradio-client-1.13.0 llamafactory-0.9.4.dev0 modelscope-1.32.0 numpy-1.26.4 peft-0.17.1 pydantic-2.10.6 pydantic-core-2.27.2 rouge-chinese-1.0.3 safetensors-0.5.3 shtab-1.8.0 transformers-4.57.1 trl-0.9.6 tyro-0.8.14\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "numpy"
        ]
       },
       "id": "1e98e3bf3881490e9b040bb90eb2be1d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.9.0+cu126)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (25.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.6)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n",
      "Downloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl (59.4 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m59.4/59.4 MB\u001B[0m \u001B[31m13.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hInstalling collected packages: bitsandbytes\n",
      "Successfully installed bitsandbytes-0.48.2\n"
     ]
    }
   ],
   "source": [
    "# 1. 安装 LLaMA-Factory 和相关依赖\n",
    "!git clone https://github.com/hiyouga/LLaMA-Factory.git\n",
    "%cd LLaMA-Factory\n",
    "!pip install -e \".[torch,metrics]\"\n",
    "!pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "# 1. 强制进入目录\n",
    "%cd /content/LLaMA-Factory\n",
    "\n",
    "# 2. 使用镜像加速下载\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "\n",
    "# 3. 禁用 W&B\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "# 4. 开始训练\n",
    "!llamafactory-cli train \\\n",
    "    --stage sft \\\n",
    "    --do_train \\\n",
    "    --model_name_or_path Qwen/Qwen2.5-Coder-1.5B-Instruct \\\n",
    "    --dataset my_cleaner_data \\\n",
    "    --template qwen \\\n",
    "    --finetuning_type lora \\\n",
    "    --lora_target all \\\n",
    "    --output_dir saves/code_cleaner_v1 \\\n",
    "    --overwrite_output_dir \\\n",
    "    --per_device_train_batch_size 2 \\\n",
    "    --gradient_accumulation_steps 8 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --logging_steps 5 \\\n",
    "    --save_steps 100 \\\n",
    "    --learning_rate 1e-4 \\\n",
    "    --num_train_epochs 5.0 \\\n",
    "    --fp16"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R2XiTXATVk6x",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764771483592,
     "user_tz": -480,
     "elapsed": 858162,
     "user": {
      "displayName": "joker",
      "userId": "03249235188414976044"
     }
    },
    "outputId": "c7473c25-a305-421e-f58b-5c9560dee77d"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/content/LLaMA-Factory\n",
      "2025-12-03 14:03:52.937764: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1764770632.958175   17378 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1764770632.964256   17378 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1764770632.980474   17378 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764770632.980509   17378 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764770632.980521   17378 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764770632.980529   17378 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-12-03 14:03:52.987184: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "[INFO|2025-12-03 14:04:01] llamafactory.hparams.parser:468 >> Process rank: 0, world size: 1, device: cuda:0, distributed training: False, compute dtype: torch.float16\n",
      "[INFO|tokenization_utils_base.py:2095] 2025-12-03 14:04:02,487 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-Coder-1.5B-Instruct/snapshots/2e1fd397ee46e1388853d2af2c993145b0f1098a/vocab.json\n",
      "[INFO|tokenization_utils_base.py:2095] 2025-12-03 14:04:02,487 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-Coder-1.5B-Instruct/snapshots/2e1fd397ee46e1388853d2af2c993145b0f1098a/merges.txt\n",
      "[INFO|tokenization_utils_base.py:2095] 2025-12-03 14:04:02,487 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-Coder-1.5B-Instruct/snapshots/2e1fd397ee46e1388853d2af2c993145b0f1098a/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2095] 2025-12-03 14:04:02,487 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2095] 2025-12-03 14:04:02,488 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2095] 2025-12-03 14:04:02,488 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-Coder-1.5B-Instruct/snapshots/2e1fd397ee46e1388853d2af2c993145b0f1098a/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2095] 2025-12-03 14:04:02,488 >> loading file chat_template.jinja from cache at None\n",
      "[INFO|tokenization_utils_base.py:2364] 2025-12-03 14:04:02,831 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|configuration_utils.py:765] 2025-12-03 14:04:04,955 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-Coder-1.5B-Instruct/snapshots/2e1fd397ee46e1388853d2af2c993145b0f1098a/config.json\n",
      "[INFO|configuration_utils.py:839] 2025-12-03 14:04:04,957 >> Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"layer_types\": [\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\"\n",
      "  ],\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"transformers_version\": \"4.57.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2095] 2025-12-03 14:04:05,477 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-Coder-1.5B-Instruct/snapshots/2e1fd397ee46e1388853d2af2c993145b0f1098a/vocab.json\n",
      "[INFO|tokenization_utils_base.py:2095] 2025-12-03 14:04:05,477 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-Coder-1.5B-Instruct/snapshots/2e1fd397ee46e1388853d2af2c993145b0f1098a/merges.txt\n",
      "[INFO|tokenization_utils_base.py:2095] 2025-12-03 14:04:05,477 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-Coder-1.5B-Instruct/snapshots/2e1fd397ee46e1388853d2af2c993145b0f1098a/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2095] 2025-12-03 14:04:05,477 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2095] 2025-12-03 14:04:05,477 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2095] 2025-12-03 14:04:05,477 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-Coder-1.5B-Instruct/snapshots/2e1fd397ee46e1388853d2af2c993145b0f1098a/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2095] 2025-12-03 14:04:05,477 >> loading file chat_template.jinja from cache at None\n",
      "[INFO|tokenization_utils_base.py:2364] 2025-12-03 14:04:05,821 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|2025-12-03 14:04:05] llamafactory.data.loader:143 >> Loading dataset code_refactor.jsonl...\n",
      "training example:\n",
      "input_ids:\n",
      "[151644, 8948, 198, 2610, 525, 1207, 16948, 11, 3465, 553, 54364, 14817, 13, 1446, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 14880, 118027, 87752, 46100, 3837, 102989, 101137, 5085, 11913, 54955, 226, 99453, 62926, 42855, 111116, 8997, 750, 1779, 38217, 5500, 5384, 3142, 982, 262, 421, 1946, 3142, 2651, 220, 16, 510, 286, 470, 3557, 198, 262, 770, 510, 286, 49109, 284, 220, 17, 198, 286, 1393, 49109, 366, 1946, 3142, 510, 310, 421, 1946, 3142, 1018, 49109, 621, 220, 15, 510, 394, 470, 3557, 198, 310, 49109, 284, 49109, 488, 220, 16, 198, 286, 470, 3007, 151645, 198, 151644, 77091, 198, 1499, 19496, 1159, 9145, 1406, 750, 374, 38217, 17918, 25, 9145, 18640, 11, 2224, 2467, 1464, 1807, 510, 262, 4210, 3973, 421, 264, 2661, 1372, 374, 264, 10250, 1372, 624, 1066, 262, 362, 10250, 1372, 374, 264, 5810, 1372, 7046, 1091, 220, 16, 429, 702, 902, 6785, 198, 262, 3429, 41214, 1008, 1091, 220, 16, 323, 5086, 624, 1066, 262, 17693, 510, 286, 1372, 25, 576, 1372, 311, 1779, 369, 8860, 2719, 13, 2980, 387, 7546, 476, 2224, 345, 394, 714, 686, 387, 16099, 311, 7546, 369, 279, 1779, 624, 1066, 262, 5184, 510, 286, 3007, 421, 279, 1372, 374, 10250, 11, 3557, 5937, 624, 1066, 262, 25311, 510, 286, 12109, 374, 38217, 7, 22, 340, 286, 3007, 198, 286, 12109, 374, 38217, 7, 16, 15, 340, 286, 3557, 198, 286, 12109, 374, 38217, 7, 16, 340, 286, 3557, 198, 286, 12109, 374, 38217, 7, 17, 340, 286, 3007, 198, 262, 3190, 262, 671, 7169, 311, 7546, 421, 2224, 11, 714, 10306, 4024, 897, 369, 12341, 198, 262, 421, 11402, 17918, 11, 2224, 982, 286, 421, 537, 1372, 2079, 31725, 3932, 310, 470, 3557, 198, 286, 1372, 284, 526, 17918, 340, 1066, 262, 671, 12518, 5109, 1969, 387, 7046, 1091, 220, 16, 198, 262, 421, 1372, 2651, 220, 16, 510, 286, 470, 3557, 7213, 262, 671, 220, 17, 374, 279, 1172, 1496, 10250, 1372, 198, 262, 421, 1372, 621, 220, 17, 510, 286, 470, 3007, 7213, 262, 671, 7418, 5109, 7046, 1091, 220, 17, 525, 537, 10250, 198, 262, 421, 1372, 1018, 220, 17, 621, 220, 15, 510, 286, 470, 3557, 7213, 262, 671, 4248, 10322, 3429, 41214, 1172, 11, 705, 311, 279, 9334, 3704, 315, 279, 1372, 198, 262, 1932, 16237, 15841, 284, 526, 17918, 3070, 220, 15, 13, 20, 8, 488, 220, 16, 198, 262, 369, 49109, 304, 2088, 7, 18, 11, 1932, 16237, 15841, 11, 220, 17, 982, 286, 421, 1372, 1018, 49109, 621, 220, 15, 510, 310, 470, 3557, 7213, 262, 470, 3007, 151645, 198]\n",
      "inputs:\n",
      "<|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "请重构以下代码，使其符合 Google Style 规范并添加文档。\n",
      "def check_prime_number(input_value):\n",
      "    if input_value <= 1:\n",
      "        return False\n",
      "    else:\n",
      "        divisor = 2\n",
      "        while divisor < input_value:\n",
      "            if input_value % divisor == 0:\n",
      "                return False\n",
      "            divisor = divisor + 1\n",
      "        return True<|im_end|>\n",
      "<|im_start|>assistant\n",
      "from typing import Union\n",
      "\n",
      "\n",
      "def is_prime(number: Union[int, float]) -> bool:\n",
      "    \"\"\"Check if a given number is a prime number.\n",
      "    \n",
      "    A prime number is a natural number greater than 1 that has no positive\n",
      "    divisors other than 1 and itself.\n",
      "    \n",
      "    Args:\n",
      "        number: The number to check for primality. Can be integer or float,\n",
      "                but will be converted to integer for the check.\n",
      "    \n",
      "    Returns:\n",
      "        True if the number is prime, False otherwise.\n",
      "    \n",
      "    Examples:\n",
      "        >>> is_prime(7)\n",
      "        True\n",
      "        >>> is_prime(10)\n",
      "        False\n",
      "        >>> is_prime(1)\n",
      "        False\n",
      "        >>> is_prime(2)\n",
      "        True\n",
      "    \"\"\"\n",
      "    # Convert to integer if float, but maintain original value for checks\n",
      "    if isinstance(number, float):\n",
      "        if not number.is_integer():\n",
      "            return False\n",
      "        number = int(number)\n",
      "    \n",
      "    # Prime numbers must be greater than 1\n",
      "    if number <= 1:\n",
      "        return False\n",
      "    \n",
      "    # 2 is the only even prime number\n",
      "    if number == 2:\n",
      "        return True\n",
      "    \n",
      "    # Even numbers greater than 2 are not prime\n",
      "    if number % 2 == 0:\n",
      "        return False\n",
      "    \n",
      "    # Check odd divisors only, up to the square root of the number\n",
      "    max_divisor = int(number ** 0.5) + 1\n",
      "    for divisor in range(3, max_divisor, 2):\n",
      "        if number % divisor == 0:\n",
      "            return False\n",
      "    \n",
      "    return True<|im_end|>\n",
      "\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 1499, 19496, 1159, 9145, 1406, 750, 374, 38217, 17918, 25, 9145, 18640, 11, 2224, 2467, 1464, 1807, 510, 262, 4210, 3973, 421, 264, 2661, 1372, 374, 264, 10250, 1372, 624, 1066, 262, 362, 10250, 1372, 374, 264, 5810, 1372, 7046, 1091, 220, 16, 429, 702, 902, 6785, 198, 262, 3429, 41214, 1008, 1091, 220, 16, 323, 5086, 624, 1066, 262, 17693, 510, 286, 1372, 25, 576, 1372, 311, 1779, 369, 8860, 2719, 13, 2980, 387, 7546, 476, 2224, 345, 394, 714, 686, 387, 16099, 311, 7546, 369, 279, 1779, 624, 1066, 262, 5184, 510, 286, 3007, 421, 279, 1372, 374, 10250, 11, 3557, 5937, 624, 1066, 262, 25311, 510, 286, 12109, 374, 38217, 7, 22, 340, 286, 3007, 198, 286, 12109, 374, 38217, 7, 16, 15, 340, 286, 3557, 198, 286, 12109, 374, 38217, 7, 16, 340, 286, 3557, 198, 286, 12109, 374, 38217, 7, 17, 340, 286, 3007, 198, 262, 3190, 262, 671, 7169, 311, 7546, 421, 2224, 11, 714, 10306, 4024, 897, 369, 12341, 198, 262, 421, 11402, 17918, 11, 2224, 982, 286, 421, 537, 1372, 2079, 31725, 3932, 310, 470, 3557, 198, 286, 1372, 284, 526, 17918, 340, 1066, 262, 671, 12518, 5109, 1969, 387, 7046, 1091, 220, 16, 198, 262, 421, 1372, 2651, 220, 16, 510, 286, 470, 3557, 7213, 262, 671, 220, 17, 374, 279, 1172, 1496, 10250, 1372, 198, 262, 421, 1372, 621, 220, 17, 510, 286, 470, 3007, 7213, 262, 671, 7418, 5109, 7046, 1091, 220, 17, 525, 537, 10250, 198, 262, 421, 1372, 1018, 220, 17, 621, 220, 15, 510, 286, 470, 3557, 7213, 262, 671, 4248, 10322, 3429, 41214, 1172, 11, 705, 311, 279, 9334, 3704, 315, 279, 1372, 198, 262, 1932, 16237, 15841, 284, 526, 17918, 3070, 220, 15, 13, 20, 8, 488, 220, 16, 198, 262, 369, 49109, 304, 2088, 7, 18, 11, 1932, 16237, 15841, 11, 220, 17, 982, 286, 421, 1372, 1018, 49109, 621, 220, 15, 510, 310, 470, 3557, 7213, 262, 470, 3007, 151645, 198]\n",
      "labels:\n",
      "from typing import Union\n",
      "\n",
      "\n",
      "def is_prime(number: Union[int, float]) -> bool:\n",
      "    \"\"\"Check if a given number is a prime number.\n",
      "    \n",
      "    A prime number is a natural number greater than 1 that has no positive\n",
      "    divisors other than 1 and itself.\n",
      "    \n",
      "    Args:\n",
      "        number: The number to check for primality. Can be integer or float,\n",
      "                but will be converted to integer for the check.\n",
      "    \n",
      "    Returns:\n",
      "        True if the number is prime, False otherwise.\n",
      "    \n",
      "    Examples:\n",
      "        >>> is_prime(7)\n",
      "        True\n",
      "        >>> is_prime(10)\n",
      "        False\n",
      "        >>> is_prime(1)\n",
      "        False\n",
      "        >>> is_prime(2)\n",
      "        True\n",
      "    \"\"\"\n",
      "    # Convert to integer if float, but maintain original value for checks\n",
      "    if isinstance(number, float):\n",
      "        if not number.is_integer():\n",
      "            return False\n",
      "        number = int(number)\n",
      "    \n",
      "    # Prime numbers must be greater than 1\n",
      "    if number <= 1:\n",
      "        return False\n",
      "    \n",
      "    # 2 is the only even prime number\n",
      "    if number == 2:\n",
      "        return True\n",
      "    \n",
      "    # Even numbers greater than 2 are not prime\n",
      "    if number % 2 == 0:\n",
      "        return False\n",
      "    \n",
      "    # Check odd divisors only, up to the square root of the number\n",
      "    max_divisor = int(number ** 0.5) + 1\n",
      "    for divisor in range(3, max_divisor, 2):\n",
      "        if number % divisor == 0:\n",
      "            return False\n",
      "    \n",
      "    return True<|im_end|>\n",
      "\n",
      "[INFO|configuration_utils.py:765] 2025-12-03 14:04:06,748 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-Coder-1.5B-Instruct/snapshots/2e1fd397ee46e1388853d2af2c993145b0f1098a/config.json\n",
      "[INFO|configuration_utils.py:839] 2025-12-03 14:04:06,748 >> Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"layer_types\": [\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\"\n",
      "  ],\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"transformers_version\": \"4.57.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|2025-12-03 14:04:06] llamafactory.model.model_utils.kv_cache:143 >> KV cache is disabled during training.\n",
      "[WARNING|logging.py:328] 2025-12-03 14:04:07,081 >> `torch_dtype` is deprecated! Use `dtype` instead!\n",
      "model.safetensors: 100% 3.09G/3.09G [00:00<00:00, 8.97MB/s]\n",
      "[INFO|modeling_utils.py:1172] 2025-12-03 14:04:07,408 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-Coder-1.5B-Instruct/snapshots/2e1fd397ee46e1388853d2af2c993145b0f1098a/model.safetensors\n",
      "[INFO|modeling_utils.py:2341] 2025-12-03 14:04:07,409 >> Instantiating Qwen2ForCausalLM model under default dtype torch.float16.\n",
      "[INFO|configuration_utils.py:986] 2025-12-03 14:04:07,411 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"use_cache\": false\n",
      "}\n",
      "\n",
      "generation_config.json: 242B [00:00, 148kB/s]\n",
      "[INFO|configuration_utils.py:941] 2025-12-03 14:04:16,117 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-Coder-1.5B-Instruct/snapshots/2e1fd397ee46e1388853d2af2c993145b0f1098a/generation_config.json\n",
      "[INFO|configuration_utils.py:986] 2025-12-03 14:04:16,117 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"repetition_penalty\": 1.1,\n",
      "  \"temperature\": 0.7,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.8\n",
      "}\n",
      "\n",
      "[INFO|dynamic_module_utils.py:423] 2025-12-03 14:04:16,399 >> Could not locate the custom_generate/generate.py inside Qwen/Qwen2.5-Coder-1.5B-Instruct.\n",
      "[INFO|2025-12-03 14:04:16] llamafactory.model.model_utils.checkpointing:143 >> Gradient checkpointing enabled.\n",
      "[INFO|2025-12-03 14:04:16] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.\n",
      "[INFO|2025-12-03 14:04:16] llamafactory.model.adapter:143 >> Upcasting trainable params to float32.\n",
      "[INFO|2025-12-03 14:04:16] llamafactory.model.adapter:143 >> Fine-tuning method: LoRA\n",
      "[INFO|2025-12-03 14:04:16] llamafactory.model.model_utils.misc:143 >> Found linear modules: up_proj,q_proj,down_proj,o_proj,k_proj,v_proj,gate_proj\n",
      "[INFO|2025-12-03 14:04:21] llamafactory.model.loader:143 >> trainable params: 9,232,384 || all params: 1,552,946,688 || trainable%: 0.5945\n",
      "[WARNING|trainer.py:906] 2025-12-03 14:04:21,727 >> The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
      "[INFO|trainer.py:749] 2025-12-03 14:04:21,890 >> Using auto half precision backend\n",
      "[WARNING|trainer.py:982] 2025-12-03 14:04:21,892 >> The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n",
      "[INFO|trainer.py:2519] 2025-12-03 14:04:22,424 >> ***** Running training *****\n",
      "[INFO|trainer.py:2520] 2025-12-03 14:04:22,424 >>   Num examples = 397\n",
      "[INFO|trainer.py:2521] 2025-12-03 14:04:22,424 >>   Num Epochs = 5\n",
      "[INFO|trainer.py:2522] 2025-12-03 14:04:22,424 >>   Instantaneous batch size per device = 2\n",
      "[INFO|trainer.py:2525] 2025-12-03 14:04:22,424 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "[INFO|trainer.py:2526] 2025-12-03 14:04:22,424 >>   Gradient Accumulation steps = 8\n",
      "[INFO|trainer.py:2527] 2025-12-03 14:04:22,424 >>   Total optimization steps = 125\n",
      "[INFO|trainer.py:2528] 2025-12-03 14:04:22,427 >>   Number of trainable parameters = 9,232,384\n",
      "{'loss': 0.6102, 'grad_norm': 0.27855953574180603, 'learning_rate': 9.974755084906502e-05, 'epoch': 0.2}\n",
      "{'loss': 0.4937, 'grad_norm': 0.22387249767780304, 'learning_rate': 9.872634363932887e-05, 'epoch': 0.4}\n",
      "{'loss': 0.4442, 'grad_norm': 0.27470025420188904, 'learning_rate': 9.693669288269372e-05, 'epoch': 0.6}\n",
      "{'loss': 0.4094, 'grad_norm': 0.2581769526004791, 'learning_rate': 9.440682244067724e-05, 'epoch': 0.8}\n",
      "{'loss': 0.4099, 'grad_norm': 0.3699715733528137, 'learning_rate': 9.117662988142138e-05, 'epoch': 1.0}\n",
      "{'loss': 0.3646, 'grad_norm': 0.2967239022254944, 'learning_rate': 8.729705727120911e-05, 'epoch': 1.2}\n",
      "{'loss': 0.3514, 'grad_norm': 0.30597439408302307, 'learning_rate': 8.282928778764783e-05, 'epoch': 1.4}\n",
      "{'loss': 0.3538, 'grad_norm': 0.33818894624710083, 'learning_rate': 7.784378082440941e-05, 'epoch': 1.6}\n",
      "{'loss': 0.3218, 'grad_norm': 0.34875738620758057, 'learning_rate': 7.241916080450163e-05, 'epoch': 1.8}\n",
      "{'loss': 0.3192, 'grad_norm': 0.33727627992630005, 'learning_rate': 6.664097722614934e-05, 'epoch': 2.0}\n",
      "{'loss': 0.3101, 'grad_norm': 0.29637420177459717, 'learning_rate': 6.0600355496102745e-05, 'epoch': 2.2}\n",
      "{'loss': 0.2749, 'grad_norm': 0.3531178832054138, 'learning_rate': 5.439255982753717e-05, 'epoch': 2.4}\n",
      "{'loss': 0.2665, 'grad_norm': 0.3337235450744629, 'learning_rate': 4.811549086650327e-05, 'epoch': 2.6}\n",
      "{'loss': 0.2843, 'grad_norm': 0.38594499230384827, 'learning_rate': 4.1868141740255823e-05, 'epoch': 2.8}\n",
      "{'loss': 0.2819, 'grad_norm': 0.40608566999435425, 'learning_rate': 3.5749036876501194e-05, 'epoch': 3.0}\n",
      "{'loss': 0.2475, 'grad_norm': 0.3736119866371155, 'learning_rate': 2.9854678214316873e-05, 'epoch': 3.2}\n",
      "{'loss': 0.2489, 'grad_norm': 0.3684626817703247, 'learning_rate': 2.4278023310924673e-05, 'epoch': 3.4}\n",
      "{'loss': 0.2384, 'grad_norm': 0.39223870635032654, 'learning_rate': 1.910701934548329e-05, 'epoch': 3.6}\n",
      "{'loss': 0.2603, 'grad_norm': 0.37915998697280884, 'learning_rate': 1.4423216139535734e-05, 'epoch': 3.8}\n",
      "{'loss': 0.2481, 'grad_norm': 0.4690929353237152, 'learning_rate': 1.0300480067608231e-05, 'epoch': 4.0}\n",
      " 80% 100/125 [10:53<02:27,  5.91s/it][INFO|trainer.py:4309] 2025-12-03 14:15:16,056 >> Saving model checkpoint to saves/code_cleaner_v1/checkpoint-100\n",
      "[INFO|configuration_utils.py:765] 2025-12-03 14:15:17,149 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-Coder-1.5B-Instruct/snapshots/2e1fd397ee46e1388853d2af2c993145b0f1098a/config.json\n",
      "[INFO|configuration_utils.py:839] 2025-12-03 14:15:17,150 >> Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"layer_types\": [\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\"\n",
      "  ],\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"transformers_version\": \"4.57.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2421] 2025-12-03 14:15:17,234 >> chat template saved in saves/code_cleaner_v1/checkpoint-100/chat_template.jinja\n",
      "[INFO|tokenization_utils_base.py:2590] 2025-12-03 14:15:17,234 >> tokenizer config file saved in saves/code_cleaner_v1/checkpoint-100/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2599] 2025-12-03 14:15:17,235 >> Special tokens file saved in saves/code_cleaner_v1/checkpoint-100/special_tokens_map.json\n",
      "{'loss': 0.2378, 'grad_norm': 0.39476343989372253, 'learning_rate': 6.803829140358237e-06, 'epoch': 4.2}\n",
      "{'loss': 0.2361, 'grad_norm': 0.38374799489974976, 'learning_rate': 3.9884076317064814e-06, 'epoch': 4.4}\n",
      "{'loss': 0.2279, 'grad_norm': 0.4194735288619995, 'learning_rate': 1.8986164206957035e-06, 'epoch': 4.6}\n",
      "{'loss': 0.2341, 'grad_norm': 0.39485856890678406, 'learning_rate': 5.674127631043025e-07, 'epoch': 4.8}\n",
      "{'loss': 0.2353, 'grad_norm': 0.4369128346443176, 'learning_rate': 1.5790535835003008e-08, 'epoch': 5.0}\n",
      "100% 125/125 [13:37<00:00,  6.34s/it][INFO|trainer.py:4309] 2025-12-03 14:17:59,721 >> Saving model checkpoint to saves/code_cleaner_v1/checkpoint-125\n",
      "[INFO|configuration_utils.py:765] 2025-12-03 14:18:00,596 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-Coder-1.5B-Instruct/snapshots/2e1fd397ee46e1388853d2af2c993145b0f1098a/config.json\n",
      "[INFO|configuration_utils.py:839] 2025-12-03 14:18:00,597 >> Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"layer_types\": [\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\"\n",
      "  ],\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"transformers_version\": \"4.57.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2421] 2025-12-03 14:18:00,684 >> chat template saved in saves/code_cleaner_v1/checkpoint-125/chat_template.jinja\n",
      "[INFO|tokenization_utils_base.py:2590] 2025-12-03 14:18:00,685 >> tokenizer config file saved in saves/code_cleaner_v1/checkpoint-125/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2599] 2025-12-03 14:18:00,685 >> Special tokens file saved in saves/code_cleaner_v1/checkpoint-125/special_tokens_map.json\n",
      "[INFO|trainer.py:2810] 2025-12-03 14:18:01,082 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 818.6547, 'train_samples_per_second': 2.425, 'train_steps_per_second': 0.153, 'train_loss': 0.3164108877182007, 'epoch': 5.0}\n",
      "100% 125/125 [13:38<00:00,  6.55s/it]\n",
      "[INFO|trainer.py:4309] 2025-12-03 14:18:01,085 >> Saving model checkpoint to saves/code_cleaner_v1\n",
      "[INFO|configuration_utils.py:765] 2025-12-03 14:18:01,940 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-Coder-1.5B-Instruct/snapshots/2e1fd397ee46e1388853d2af2c993145b0f1098a/config.json\n",
      "[INFO|configuration_utils.py:839] 2025-12-03 14:18:01,940 >> Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"layer_types\": [\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\"\n",
      "  ],\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"transformers_version\": \"4.57.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2421] 2025-12-03 14:18:02,030 >> chat template saved in saves/code_cleaner_v1/chat_template.jinja\n",
      "[INFO|tokenization_utils_base.py:2590] 2025-12-03 14:18:02,033 >> tokenizer config file saved in saves/code_cleaner_v1/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2599] 2025-12-03 14:18:02,033 >> Special tokens file saved in saves/code_cleaner_v1/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =        5.0\n",
      "  total_flos               =  7135194GF\n",
      "  train_loss               =     0.3164\n",
      "  train_runtime            = 0:13:38.65\n",
      "  train_samples_per_second =      2.425\n",
      "  train_steps_per_second   =      0.153\n",
      "[INFO|modelcard.py:456] 2025-12-03 14:18:02,232 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "# 1. 设置路径\n",
    "base_model_path = \"Qwen/Qwen2.5-Coder-1.5B-Instruct\"\n",
    "lora_path = \"saves/code_cleaner_v1\"\n",
    "\n",
    "print(\"正在加载基座模型 (Qwen)...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_path,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "print(\"正在挂载你的 LoRA 权重...\")\n",
    "model = PeftModel.from_pretrained(model, lora_path)\n",
    "\n",
    "# 2. 准备一个超级烂的代码案例\n",
    "bad_code = \"\"\"\n",
    "def calc(x,y):\n",
    "    # calc things\n",
    "    a = x*y\n",
    "    if a > 10: return a\n",
    "    return 0\n",
    "\"\"\"\n",
    "\n",
    "# 3. 构造提示词\n",
    "prompt = f\"\"\"请重构以下代码，使其符合 Google Style 规范并添加文档。\n",
    "{bad_code}\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# 4. 让模型生成\n",
    "print(\"\\n模型正在思考并重构代码...\\n\" + \"=\"*50)\n",
    "generated_ids = model.generate(\n",
    "    model_inputs.input_ids,\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.5\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "# 5. 打印结果\n",
    "print(response)\n",
    "print(\"=\"*50)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "REm5vdLvlqLF",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764771707489,
     "user_tz": -480,
     "elapsed": 34019,
     "user": {
      "displayName": "joker",
      "userId": "03249235188414976044"
     }
    },
    "outputId": "40fd4c74-8c7d-405a-c769-9bffa5710730"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "⏳ 正在加载基座模型 (Qwen)...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "🔗 正在挂载你的 LoRA 权重...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "🤖 模型正在思考并重构代码...\n",
      "==================================================\n",
      "def calculate_product_and_check_limit(num1: float, num2: float) -> float:\n",
      "    \"\"\"Calculate the product of two numbers and check if it exceeds a limit.\n",
      "    \n",
      "    Args:\n",
      "        num1: First number for multiplication.\n",
      "        num2: Second number for multiplication.\n",
      "        \n",
      "    Returns:\n",
      "        Product of num1 and num2 if it's less than or equal to 10; otherwise,\n",
      "        returns 0.\n",
      "        \n",
      "    Examples:\n",
      "        >>> calculate_product_and_check_limit(3, 4)\n",
      "        12.0\n",
      "        >>> calculate_product_and_check_limit(5, 6)\n",
      "        0.0\n",
      "    \"\"\"\n",
      "    product = num1 * num2\n",
      "    \n",
      "    if product <= 10:\n",
      "        return product\n",
      "    \n",
      "    return 0.0\n",
      "==================================================\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import shutil\n",
    "from google.colab import files\n",
    "\n",
    "# 1. 压缩文件夹\n",
    "print(\"正在打包模型权重...\")\n",
    "shutil.make_archive(\"my_code_cleaner_model\", 'zip', \"saves/code_cleaner_v1\")\n",
    "\n",
    "# 2. 触发下载\n",
    "print(\"⬇开始下载到本地 (约 20MB)...\")\n",
    "files.download(\"my_code_cleaner_model.zip\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "id": "5eCOBIAznFg3",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764774398518,
     "user_tz": -480,
     "elapsed": 16238,
     "user": {
      "displayName": "joker",
      "userId": "03249235188414976044"
     }
    },
    "outputId": "1135831d-6f73-4695-cb66-5443bf9a30ad"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "📦 正在打包模型权重...\n",
      "⬇️ 开始下载到本地 (约 20MB)...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ],
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ],
      "application/javascript": [
       "download(\"download_c7105648-61d8-4a18-89f7-4ce13b9d0763\", \"my_code_cleaner_model.zip\", 244653985)"
      ]
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# 1. 挂载 Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# 2. 定义源文件和目标路径\n",
    "source_file = \"my_code_cleaner_model.zip\"\n",
    "destination_path = \"/content/drive/MyDrive/my_code_cleaner_model.zip\"\n",
    "\n",
    "# 3. 内网复制\n",
    "print(f\"在把 {source_file} 搬运到 Google Drive...\")\n",
    "shutil.copy(source_file, destination_path)\n",
    "\n",
    "print(f\"成功！请去你的 Google Drive 网盘根目录查看。\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zeNFOJLtxH24",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764774904874,
     "user_tz": -480,
     "elapsed": 11501,
     "user": {
      "displayName": "joker",
      "userId": "03249235188414976044"
     }
    },
    "outputId": "a3dfd116-96da-4fdf-b857-3b509ec32321"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n",
      "🚀 正在把 my_code_cleaner_model.zip 搬运到 Google Drive...\n",
      "✅ 成功！请去你的 Google Drive 网盘根目录查看，文件已经在那里了。\n"
     ]
    }
   ]
  }
 ]
}
